---
title: "Moringa week 13"
author: "Simon Mmari"
date: '2022-03-26'
output: html_document
---
# Defining the question

## Specifying the Data Analytic Question.
``` Classifying customers by studying and understanding their behavior from data collected over the past year .```
## Defining the metric of success
``` The project will be a success when we are able to identify the customers who are most likely to complete an e-commerce transaction by studying their online behavior.```
## Understanding the context
``` Kira Plastinina is a Russian brand that is sold through a defunct chain of retail stores in Russia, Ukraine, Kazakhstan, Belarus, China, Philippines, and Armenia. The brand’s Sales and Marketing team would like to understand their customer’s behavior from data that they have collected over the past year. More specifically, they would like to learn the characteristics of customer groups. Findings from the study will help inform the team on formulating  marketing and sales strategies for the brand. ```
## Recording the experimental design
```The process will entail:```
* Problem Definition
* Data Sourcing
* Check the Data
* Perform Data Cleaning
* Perform Exploratory Data Analysis  (Univariate, Bivariate & Multivariate)
* Implement the Solution
* Challenge the Solution

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Loading Data

# Install packages needed 
#install.packages("naniar")
#pkgs <- c("factoextra",  "NbClust")
#install.packages(pkgs)
#install.packages("superml")
```

```{r}
library (tidyr)
library(naniar)
library (ggplot2)
library (e1071)
library (corrplot)
library(factoextra)
library(NbClust)
library(superml)
library(ggplot2)
```

# Loading Dataset.
```{r}
sp <- read.csv("http://bit.ly/EcommerceCustomersDataset")
sp
```

## Previewing top Datastet
```{r}
head(sp)

```

# Checking the dataset.
```{r}
dim(sp)

```
````Dataset has 18 columns and 12,30 rows.````

```{r}
colSums(is.na(sp))
```
```` Dataset has numerous null values.````

```{r}

sp1 <- na.omit(sp) 

```
Null values are dropped this is because they will interefere with my analysis.


```{r}
# Confirming if Null values are droppped.
#
colSums(is.na(sp1))

```

```{r}
sp2 <- sp1[duplicated(sp1),]
sp2

```
````Dataset has no duplicated values.````


```{r}
#convert categorical columns to factors
sp1[,11:18] <- sapply(sp1[,11:18], as.factor)
head(sp1)
```

```{r}

#checking for outliers
#list(colnames(ec))
boxplot(sp1$Administrative, main= 'Boxplot of administrative web pages', xlab='administrative', ylab='value')
boxplot(sp1$Administrative_Duration, main= 'Boxplot of administrative web pages duration', xlab='administrative duration', ylab='value')
boxplot(sp1$Informational, main= 'Boxplot of informational web pages', xlab='informational', ylab='value')
boxplot(sp1$Informational_Duration, main= 'Boxplot of informational web pages duration', xlab='informational duration', ylab='value')
boxplot(sp1$ProductRelated, main= 'Boxplot of product related web pages', xlab='product related', ylab='value')
boxplot(sp$ProductRelated_Duration, main= 'Boxplot of product related web pages duration', xlab='product related duration', ylab='value')
boxplot(sp1$BounceRates, main= 'Boxplot of bounce rates', xlab='bounce rates', ylab='value')
boxplot(sp1$ExitRates, main= 'Boxplot of exit rates', xlab='exit rates', ylab='value')
boxplot(sp1$PageValues, main= 'Boxplot of page values', xlab='page values', ylab='value')
boxplot(sp1$SpecialDay, main= 'Boxplot of special day', xlab='special day', ylab='value')

```

```There are outliers in every column but we wunt remove them as they might give insight to the project.```


# Univariate Analysis.

## Measures of central tendancy
```{r}
#finding the mean
mean <- colMeans(sp1[sapply(sp1, is.numeric)])
mean
#Finding the median
#loading the tidyverse and robustbase(for the colMedians function) libraries
library(robustbase)
library(tidyverse)
median <- ec%>%
  select_if(is.numeric) %>%
  as.matrix()%>%
  colMedians()
print(median)
#Finding the mode
mode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
```
```{r}
#finding the minimum
min(sp1$Administrative)
min(sp1$Administrative_Duration)
min(sp1$Informational)
min(sp1$Informational_Duration)
min(sp1$ProductRelated)
min(sp1$ProductRelated_Duration)
min(sp1$BounceRates)
min(sp1$ExitRates)
min(sp1$PageValues)
min(sp1$SpecialDay)
```
```{r}

#finding the maximum
max(sp1$Administrative)
max(sp1$Administrative_Duration)
max(sp1$Informational)
max(sp1$Informational_Duration)
max(sp1$ProductRelated)
max(sp1$ProductRelated_Duration)
max(sp1$BounceRates)
max(sp1$ExitRates)
max(sp1$PageValues)
max(sp1$SpecialDay)

```
### Categorical columns
```{r}
#colnames(sp1)
#frequency table of month
month.freq <- table(sp1$Month)
sort(month.freq, decreasing = TRUE)[1:5]
#Bar chart to show frequency distribution of months 
options(repr.plot.width = 10, repr.plot.height = 10)
barplot(c(month.freq), main="A barchart of months.",
        xlab="months",
        ylab="frequency",
        cex.main=2, cex.lab=1.7,cex.sub=1.2,
        col=c("orange"))

```
```` May has most web page visits amongst the 12 months, February being the least.````


```{r}

#frequency table of Operating systems
os.freq <- table(sp1$OperatingSystems)
sort(os.freq, decreasing = TRUE)[1:5]
#Bar chart to show frequency distribution of operating systems
options(repr.plot.width = 10, repr.plot.height = 10)
barplot(c(os.freq), main="A barchart of operating systems.",
        xlab="operating systems",
        ylab="frequency",
        cex.main=2, cex.lab=1.7,cex.sub=1.2,
        col=c("green"))

```
````The mostly used Operating Systems is the type 2.````


```{r}

#frequency table of region
region.freq <- table(sp1$Region)
sort(region.freq, decreasing = TRUE)[1:5]
#Bar chart to show frequency distribution of regions 
options(repr.plot.width = 10, repr.plot.height = 10)
barplot(c(region.freq), main="A barchart of regions.",
        xlab="region",
        ylab="frequency",
        cex.main=2, cex.lab=1.7,cex.sub=1.2,
        col=c("red"))
```
````It is from Region one where more Data was gathered than the other 9.````


```{r}
#frequency table of browser
browser.freq <- table(sp1$Browser)
sort(browser.freq, decreasing = TRUE)[1:5]
#Bar chart to show frequency distribution of browsers 
options(repr.plot.width = 10, repr.plot.height = 10)
barplot(c(browser.freq), main="A barchart of browser.",
        xlab="browser",
        ylab="frequency",
        cex.main=2, cex.lab=1.7,cex.sub=1.2,
        col=c("blue"))

````
````Browser type 2 is the mostly used browser.````


```{r}
#frequency table of traffic type
traffic.freq <- table(sp1$TrafficType)
sort(traffic.freq, decreasing = TRUE)[1:5]
#Bar chart to show frequency distribution of traffic type 
options(repr.plot.width = 10, repr.plot.height = 10)
barplot(c(traffic.freq), main="A barchart of traffic type.",
        xlab="traffic type",
        ylab="frequency",
        cex.main=2, cex.lab=1.7,cex.sub=1.2,
        col=c("yellow"))
````

```{r}
#frequency table of revenue
rev.freq <- table(sp1$Revenue)
sort(rev.freq, decreasing = TRUE)[1:5]
#Bar chart to show frequency distribution of revenue 
options(repr.plot.width = 10, repr.plot.height = 10)
barplot(c(rev.freq), main="A barchart of revenue.",
        xlab="revenue",
        ylab="frequency",
        cex.main=2, cex.lab=1.7,cex.sub=1.2,
        col=c("cyan", "black"))
```

```{r}
#creating a histogram of administrative variable.
hist(sp1$Administrative, col=c("pink"))
```

```{r}
#creating a histogram of administrative duration variable
hist(sp1$Administrative_Duration, col=c("green"))
```

```{r}
#creating a histogram of bounce rates variable
hist(sp1$BounceRates, col=c("red1"))

```

```{r}
#creating a histogram of special day variable
hist(sp1$SpecialDay, col=c("tomato1"))
```

# Bivariate analysis
```{r}
#finding the covariance
cov <- cov(sp1[,unlist(lapply(sp1, is.numeric))])
cov
```

```{r}

#Finding correlation
cor <- cor(sp1[, unlist(lapply(sp1, is.numeric))])
cor
```

```{r}
#selecting the true values from the revenue column
revenue <- sp1[sp1$Revenue == 'TRUE',]
head(revenue)
dim(revenue)
```

```{r}
#comparison between month and revenue brought in
#colnames(ec)
#frequency table of month
month1.freq <- table(revenue$Month)
sort(month1.freq, decreasing = TRUE)[1:5]
#Bar chart to show frequency distribution of months 
options(repr.plot.width = 10, repr.plot.height = 10)
barplot(c(month1.freq), main="A barchart of months.",
        xlab="months",
        ylab="revenue",
        cex.main=2, cex.lab=1.7,cex.sub=1.2,
        col=c("chartreuse1"))
```

```{r}
#comparison between operating systems and revenue brought in
#frequency table of Operating systems
os1.freq <- table(revenue$OperatingSystems)
sort(os1.freq, decreasing = TRUE)[1:5]
#Bar chart to show frequency distribution of operating systems
options(repr.plot.width = 10, repr.plot.height = 10)
barplot(c(os1.freq), main="A barchart of operating systems.",
        xlab="operating systems",
        ylab="revenue",
        cex.main=2, cex.lab=1.7,cex.sub=1.2,
        col=c("aquamarine"))
```

```{r}
#comparison between region and revenue brought in
#frequency table of region
region1.freq <- table(revenue$Region)
sort(region1.freq, decreasing = TRUE)[1:5]
#Bar chart to show frequency distribution of regions 
options(repr.plot.width = 10, repr.plot.height = 10)
barplot(c(region1.freq), main="A barchart of regions.",
        xlab="region",
        ylab="revenue",
        cex.main=2, cex.lab=1.7,cex.sub=1.2,
        col=c("cyan"))

```

```{r}

#comparison between weekend and revenue brought in
#frequency table of weekend
weekend1.freq <- table(revenue$Weekend)
sort(weekend1.freq, decreasing = TRUE)[1:5]
#Bar chart to show frequency distribution of weekend 
options(repr.plot.width = 10, repr.plot.height = 10)
barplot(c(weekend1.freq), main="A barchart of weekend.",
        xlab="weekend",
        ylab="revenue",
        cex.main=2, cex.lab=1.7,cex.sub=1.2,
        col=c("blue"))
```
# Multivariate Analysis.
```{r}
pairs(sp1[, 1:10])
```

```{r}
#Plotting a heat map using the correlation matrix
heatmap(x=cor, symm=TRUE)

```

```{r}
#Plotting a correlogram
library('corrplot')
corrplot(cor, type='upper', order='hclust', tl.color='black', tl.srt=45)

```
# Modelling.
## KNN

```{r}
#removing the revenue column
sp1.new <- sp1[,1:17]
head(sp1.new)
sp1.rev <- sp1$Revenue
sp1.new[,12:15] <- sapply(sp1.new[,12:15], as.character)
sp1.new[,12:15] <- sapply(sp1.new[,12:15], as.numeric)
head(sp1.new)
```

```{r}
#one hot encoding factor columns
library(caret)
dmy = dummyVars(" ~ .", data = sp1.new)
sp1.encod = data.frame(predict(dmy, newdata = sp1.new))
str(sp1.encod)
dim(sp1.encod)
```

```{r}
#normalizing data
normalize <- function(x){
  return ((x-min(x)) / (max(x)-min(x)))
}
normalize(sp1.encod)

```

```{r}
# Applying the K-means clustering algorithm with no. of centroids(k)=3
# ---
# 
result<- kmeans(sp1.encod,10) 
# Previewing the no. of records in each cluster
# 
result$size
result$cluster
result$centers

```


# Hierarchical Clustering.

```{r}
# Loading the data set
# ---
#
data("USArrests")
```


```{r}
z <- dist(sp1.encod, method = "euclidean")

```


```{r}
# We then hierarchical clustering using the Ward's method
# ---
# 
res.hc <- hclust(z, method = "ward.D2" )
res.hc
```

```{r}

# plotting the obtained dendrogram
# ---
# 
plot(res.hc, cex = 0.6, hang = -1)

```

## DBSCAN
```{r}
install.packages("dbscan")

```
```{r}
# Loading the recquired package.
library("dbscan")

```

```{r}
np<-dbscan(sp1.encod,eps=0.4,MinPts = 4)
np
```

```{r}
# Plotting our clusters as shown

hullplot(sp1.encod,np$cluster)

```

# Conclusion

> Kmeans performed better than hierachical clustering in this study as it can handle larger datasets as compared to hierachical clustering.




# Recommendations


> Region 1, is where the returning visitors generate the most revenue. The company should consider looking into exploring  these features further to maximize the income brought in.

> I'd recommed the use of Kmeans clustering as it can handle large datasets better that hierachial clustering.

# Follow up questions

## Did we have the right data?
> Yes we did. Our data set had a good number of variables that helped us study the individuals and determine who was likely to finish an ecommerce transaction.

## Do we need other data to answer our question?
> Not necessarily, however further research is needed to help gain deeper insight on the study.

## Did we have the right question?
> Classifying customers by studying and understanding their behavior from data collected over the past year while also learning the characteristics of customer groups. We were able to do that by analysing the given dataset.
